"""
è®­ç»ƒSeq2Seqæ¨¡å‹ç”¨äºNER+REä»»åŠ¡ï¼ˆæ”¯æŒLoRAå¾®è°ƒï¼‰
"""
import os
import sys
import torch

# åœ¨å¯¼å…¥transformersä¹‹å‰ï¼Œç¦ç”¨ç‰ˆæœ¬æ£€æŸ¥ï¼ˆä¸´æ—¶è§£å†³æ–¹æ¡ˆï¼‰
# è¿™å¯ä»¥ç»•è¿‡PyTorch 2.6çš„ç‰ˆæœ¬è¦æ±‚
def _patch_transformers_version_check():
    """ä¿®è¡¥transformersçš„ç‰ˆæœ¬æ£€æŸ¥"""
    try:
        # æ–¹æ³•1: ä¿®æ”¹ import_utils æ¨¡å—
        import transformers.utils.import_utils as import_utils
        if hasattr(import_utils, 'check_torch_load_is_safe'):
            # ä¿å­˜åŸå§‹å‡½æ•°ï¼ˆå¦‚æœéœ€è¦ï¼‰
            _original_check = import_utils.check_torch_load_is_safe
            
            # åˆ›å»ºä¸€ä¸ªç»•è¿‡ç‰ˆæœ¬æ£€æŸ¥çš„å‡½æ•°
            def _patched_check():
                """ç»•è¿‡torch.loadçš„ç‰ˆæœ¬æ£€æŸ¥"""
                pass  # ä¸åšä»»ä½•æ£€æŸ¥
            
            # æ›¿æ¢å‡½æ•°
            import_utils.check_torch_load_is_safe = _patched_check
            return True
    except Exception as e:
        # å¦‚æœè¡¥ä¸å¤±è´¥ï¼Œç»§ç»­æ‰§è¡Œ
        pass
    return False

# åº”ç”¨è¡¥ä¸
_patch_transformers_version_check()

from transformers import (
    T5ForConditionalGeneration,
    T5Tokenizer,
    Trainer,
    TrainingArguments,
    DataCollatorForSeq2Seq
)
from torch.utils.data import DataLoader

# æ·»åŠ çˆ¶ç›®å½•åˆ°è·¯å¾„
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from models.dataset import Seq2SeqDataset
from config import Config

# å¯¼å…¥LoRAç›¸å…³åº“
try:
    from peft import LoraConfig, get_peft_model, TaskType, PeftModel
    PEFT_AVAILABLE = True
except ImportError:
    PEFT_AVAILABLE = False
    print("âš ï¸  è­¦å‘Š: peftåº“æœªå®‰è£…ï¼Œå°†ä½¿ç”¨å…¨é‡å¾®è°ƒ")
    print("   å®‰è£…å‘½ä»¤: pip install peft")


def train_model():
    """è®­ç»ƒSeq2Seqæ¨¡å‹"""
    
    config = Config()
    
    # ç¡®ä¿ç›®å½•å­˜åœ¨
    os.makedirs(config.MODEL_DIR, exist_ok=True)
    os.makedirs(config.LOG_DIR, exist_ok=True)
    
    print("=" * 50)
    print("å¼€å§‹è®­ç»ƒSeq2Seqæ¨¡å‹")
    print("=" * 50)
    print(f"æ¨¡å‹: {config.MODEL_NAME}")
    print(f"è®­ç»ƒæ•°æ®: {config.TRAIN_DATA_PATH}")
    print(f"è¾“å‡ºç›®å½•: {config.MODEL_DIR}")
    print("=" * 50)
    
    # 1. åŠ è½½tokenizerå’Œæ¨¡å‹
    print("\n1. åŠ è½½tokenizerå’Œæ¨¡å‹...")
    # ç›´æ¥ä½¿ç”¨å¤‡ç”¨æ–¹æ³•åŠ è½½æ¨¡å‹ï¼ˆç»•è¿‡transformersç‰ˆæœ¬æ£€æŸ¥ï¼‰
    tokenizer = T5Tokenizer.from_pretrained(config.MODEL_NAME)
    
    # ç›´æ¥åŠ è½½é…ç½®å’Œæƒé‡ï¼ˆç»•è¿‡å®‰å…¨æ£€æŸ¥ï¼‰
    from transformers import T5Config
    
    print("   åŠ è½½æ¨¡å‹é…ç½®...")
    model_config = T5Config.from_pretrained(config.MODEL_NAME)
    # åˆ›å»ºæ¨¡å‹
    model = T5ForConditionalGeneration(model_config)
    
    # ç›´æ¥åŠ è½½æƒé‡
    model_path = os.path.join(config.MODEL_NAME, "pytorch_model.bin")
    if os.path.exists(model_path):
        print(f"   ä» {model_path} åŠ è½½æƒé‡...")
        # ä½¿ç”¨ torch.loadï¼Œè®¾ç½® weights_only=False æ¥ç»•è¿‡å®‰å…¨æ£€æŸ¥
        try:
            state_dict = torch.load(model_path, map_location="cpu", weights_only=False)
        except TypeError:
            # å¦‚æœ weights_only å‚æ•°ä¸æ”¯æŒï¼Œä½¿ç”¨æ—§æ–¹æ³•
            state_dict = torch.load(model_path, map_location="cpu")
        model.load_state_dict(state_dict, strict=False)
        print("   âœ… æ¨¡å‹åŠ è½½æˆåŠŸ")
    else:
        raise FileNotFoundError(f"æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {model_path}")
    
    # è®¾ç½®pad_tokenï¼ˆæŸäº›æ¨¡å‹å¯èƒ½æ²¡æœ‰ï¼‰
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
        model.config.pad_token_id = model.config.eos_token_id
    
    # è·å–åŸå§‹æ¨¡å‹å‚æ•°é‡
    total_params = sum(p.numel() for p in model.parameters())
    trainable_params_before = sum(p.numel() for p in model.parameters() if p.requires_grad)
    
    # 2. é…ç½®LoRAï¼ˆå¦‚æœå¯ç”¨ï¼‰
    use_lora = config.USE_LORA and PEFT_AVAILABLE
    if use_lora:
        print("\n2. é…ç½®LoRAå¾®è°ƒ...")
        print(f"   LoRA Rank (r): {config.LORA_R}")
        print(f"   LoRA Alpha: {config.LORA_ALPHA}")
        print(f"   LoRA Dropout: {config.LORA_DROPOUT}")
        print(f"   ç›®æ ‡æ¨¡å—: {config.LORA_TARGET_MODULES}")
        
        # åˆ›å»ºLoRAé…ç½®
        lora_config = LoraConfig(
            task_type=TaskType.SEQ_2_SEQ_LM,  # T5æ˜¯åºåˆ—åˆ°åºåˆ—ä»»åŠ¡
            r=config.LORA_R,
            lora_alpha=config.LORA_ALPHA,
            lora_dropout=config.LORA_DROPOUT,
            target_modules=config.LORA_TARGET_MODULES,
            bias=config.LORA_BIAS,
        )
        
        # åº”ç”¨LoRAåˆ°æ¨¡å‹
        model = get_peft_model(model, lora_config)
        
        # æ‰“å°å¯è®­ç»ƒå‚æ•°ä¿¡æ¯
        trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
        print(f"\nâœ… LoRAé…ç½®å®Œæˆ")
        print(f"   åŸå§‹æ¨¡å‹å‚æ•°: {total_params/1e6:.2f}M")
        print(f"   å¯è®­ç»ƒå‚æ•°: {trainable_params/1e6:.2f}M ({trainable_params/total_params*100:.2f}%)")
        print(f"   å‚æ•°å‡å°‘: {(1 - trainable_params/total_params)*100:.2f}%")
        
        # æ‰“å°LoRAæ¨¡å‹ç»“æ„
        model.print_trainable_parameters()
        
        # ç§»åŠ¨æ¨¡å‹åˆ°è®¾å¤‡ï¼ˆå¼ºåˆ¶ä½¿ç”¨CUDAæˆ–CPUï¼Œä¸ä½¿ç”¨MPSï¼‰
        device = "cuda" if torch.cuda.is_available() else "cpu"
        print(f"   ä½¿ç”¨è®¾å¤‡: {device.upper()}")
        model = model.to(device)
    else:
        if config.USE_LORA and not PEFT_AVAILABLE:
            print("\nâš ï¸  LoRAå·²å¯ç”¨ä½†peftåº“æœªå®‰è£…ï¼Œä½¿ç”¨å…¨é‡å¾®è°ƒ")
        else:
            print("\nğŸ“ ä½¿ç”¨å…¨é‡å¾®è°ƒæ¨¡å¼")
        print(f"âœ… æ¨¡å‹åŠ è½½å®Œæˆ")
        print(f"   Vocab size: {len(tokenizer)}")
        print(f"   Model parameters: {total_params/1e6:.1f}M")
        print(f"   å¯è®­ç»ƒå‚æ•°: {trainable_params_before/1e6:.1f}M")
        
        # ç§»åŠ¨æ¨¡å‹åˆ°è®¾å¤‡ï¼ˆå¼ºåˆ¶ä½¿ç”¨CUDAæˆ–CPUï¼Œä¸ä½¿ç”¨MPSï¼‰
        device = "cuda" if torch.cuda.is_available() else "cpu"
        print(f"   ä½¿ç”¨è®¾å¤‡: {device.upper()}")
        model = model.to(device)
    
    # 3. å‡†å¤‡æ•°æ®é›†
    print("\n3. å‡†å¤‡æ•°æ®é›†...")
    train_dataset = Seq2SeqDataset(
        config.TRAIN_DATA_PATH,
        tokenizer,
        max_length=config.MAX_LENGTH,
        max_target_length=config.MAX_TARGET_LENGTH
    )
    
    # å¦‚æœæœ‰éªŒè¯é›†ï¼ŒåŠ è½½éªŒè¯é›†
    dev_dataset = None
    if os.path.exists(config.DEV_DATA_PATH):
        print(f"   å‘ç°éªŒè¯é›†: {config.DEV_DATA_PATH}")
        dev_dataset = Seq2SeqDataset(
            config.DEV_DATA_PATH,
            tokenizer,
            max_length=config.MAX_LENGTH,
            max_target_length=config.MAX_TARGET_LENGTH
        )
    
    # 4. æ•°æ®æ•´ç†å™¨
    data_collator = DataCollatorForSeq2Seq(
        tokenizer=tokenizer,
        model=model,
        padding=True
    )
    
    # 5. è®­ç»ƒå‚æ•°ï¼ˆLoRAå¾®è°ƒæ—¶å­¦ä¹ ç‡å¯ä»¥é€‚å½“æé«˜ï¼‰
    print("\n4. è®¾ç½®è®­ç»ƒå‚æ•°...")
    learning_rate = config.LEARNING_RATE
    if use_lora:
        # LoRAå¾®è°ƒæ—¶ï¼Œå­¦ä¹ ç‡é€šå¸¸æ¯”å…¨é‡å¾®è°ƒé«˜5-10å€
        learning_rate = config.LEARNING_RATE * 5
        print(f"   LoRAå¾®è°ƒæ¨¡å¼ï¼Œå­¦ä¹ ç‡è°ƒæ•´ä¸º: {learning_rate}")
    # æ£€æŸ¥æ˜¯å¦ä½¿ç”¨CUDA GPU
    use_cuda = torch.cuda.is_available()
    
    training_args = TrainingArguments(
        output_dir=config.MODEL_DIR,
        num_train_epochs=config.NUM_EPOCHS,
        per_device_train_batch_size=config.BATCH_SIZE,
        per_device_eval_batch_size=config.BATCH_SIZE,
        gradient_accumulation_steps=getattr(config, 'GRADIENT_ACCUMULATION_STEPS', 1),  # æ¢¯åº¦ç´¯ç§¯
        learning_rate=learning_rate,
        warmup_steps=config.WARMUP_STEPS,
        logging_dir=config.LOG_DIR,
        logging_steps=config.LOGGING_STEPS,
        save_steps=config.SAVE_STEPS,
        eval_strategy="steps" if dev_dataset else "no",  # evaluation_strategyå·²é‡å‘½åä¸ºeval_strategy
        eval_steps=config.EVAL_STEPS if dev_dataset else None,
        save_total_limit=3,  # åªä¿ç•™æœ€è¿‘3ä¸ªæ¨¡å‹
        load_best_model_at_end=True if dev_dataset else False,
        metric_for_best_model="loss" if dev_dataset else None,
        greater_is_better=False if dev_dataset else None,
        fp16=use_cuda,  # å¦‚æœä½¿ç”¨CUDA GPUï¼Œå¯ç”¨æ··åˆç²¾åº¦
        bf16=False,  # å¯é€‰ï¼šå¦‚æœGPUæ”¯æŒbf16ï¼Œå¯ä»¥å¯ç”¨
        dataloader_num_workers=4 if use_cuda else 0,  # CUDAå¯ä»¥ä½¿ç”¨å¤šè¿›ç¨‹åŠ è½½æ•°æ®
        dataloader_pin_memory=True if use_cuda else False,  # CUDAæ”¯æŒpin_memoryåŠ é€Ÿ
    )
    
    if use_cuda:
        print(f"   âœ… æ£€æµ‹åˆ°CUDAè®¾å¤‡ï¼Œä½¿ç”¨GPUåŠ é€Ÿ:")
        print(f"      Batch size: {config.BATCH_SIZE}")
        print(f"      æ¢¯åº¦ç´¯ç§¯æ­¥æ•°: {getattr(config, 'GRADIENT_ACCUMULATION_STEPS', 1)}")
        print(f"      ç­‰æ•ˆbatch size: {config.BATCH_SIZE * getattr(config, 'GRADIENT_ACCUMULATION_STEPS', 1)}")
        print(f"      æœ€å¤§è¾“å…¥é•¿åº¦: {config.MAX_LENGTH}")
        print(f"      æœ€å¤§è¾“å‡ºé•¿åº¦: {config.MAX_TARGET_LENGTH}")
        print(f"      FP16æ··åˆç²¾åº¦: å¯ç”¨")
    
    # 6. åˆ›å»ºTrainer
    print("\n5. åˆ›å»ºTrainer...")
    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=train_dataset,
        eval_dataset=dev_dataset,
        data_collator=data_collator,
    )
    
    # 7. å¼€å§‹è®­ç»ƒ
    print("\n6. å¼€å§‹è®­ç»ƒ...")
    print("=" * 50)
    trainer.train()
    
    # 8. ä¿å­˜æœ€ç»ˆæ¨¡å‹
    print("\n7. ä¿å­˜æœ€ç»ˆæ¨¡å‹...")
    final_model_path = os.path.join(config.MODEL_DIR, 'final_model')
    
    if use_lora:
        # LoRAæ¨¡å¼ä¸‹ï¼Œä¿å­˜LoRAæƒé‡å’ŒåŸºç¡€æ¨¡å‹
        print("   ä¿å­˜LoRAæƒé‡...")
        trainer.save_model(final_model_path)  # è¿™ä¼šä¿å­˜LoRAæƒé‡
        tokenizer.save_pretrained(final_model_path)
        
        # å¯é€‰ï¼šåˆå¹¶LoRAæƒé‡åˆ°åŸºç¡€æ¨¡å‹ï¼ˆç”¨äºæ¨ç†ï¼‰
        print("   åˆå¹¶LoRAæƒé‡åˆ°åŸºç¡€æ¨¡å‹...")
        merged_model_path = os.path.join(config.MODEL_DIR, 'final_model_merged')
        os.makedirs(merged_model_path, exist_ok=True)
        
        # åŠ è½½åŸºç¡€æ¨¡å‹
        base_model = T5ForConditionalGeneration.from_pretrained(config.MODEL_NAME)
        # åŠ è½½LoRAæƒé‡
        model = PeftModel.from_pretrained(base_model, final_model_path)
        # åˆå¹¶æƒé‡
        merged_model = model.merge_and_unload()
        # ä¿å­˜åˆå¹¶åçš„æ¨¡å‹
        merged_model.save_pretrained(merged_model_path)
        tokenizer.save_pretrained(merged_model_path)
        
        print(f"âœ… è®­ç»ƒå®Œæˆï¼")
        print(f"   LoRAæƒé‡å·²ä¿å­˜åˆ°: {final_model_path}")
        print(f"   åˆå¹¶æ¨¡å‹å·²ä¿å­˜åˆ°: {merged_model_path}")
    else:
        # å…¨é‡å¾®è°ƒæ¨¡å¼ï¼Œç›´æ¥ä¿å­˜
        trainer.save_model(final_model_path)
        tokenizer.save_pretrained(final_model_path)
        print(f"âœ… è®­ç»ƒå®Œæˆï¼")
        print(f"   æ¨¡å‹å·²ä¿å­˜åˆ°: {final_model_path}")
    
    print("=" * 50)


if __name__ == '__main__':
    train_model()

