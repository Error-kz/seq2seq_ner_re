"""
æœ€ç»ˆç¨³å®šç‰ˆ Seq2Seq Datasetï¼ˆNER + RE ä¸“ç”¨ï¼‰

è®¾è®¡ç›®æ ‡ï¼š
- ç»ä¸å‡ºç°ç©º label
- ä¸åœ¨ Dataset é˜¶æ®µ padding target
- token æ•°ç¨³å®šï¼ŒFP16 + LoRA ä¸ç‚¸
- é€‚é… Trainer + DataCollatorForSeq2Seq
"""

import os
import sys
import numpy as np
from torch.utils.data import Dataset


class Seq2SeqDataset(Dataset):
    """ç¨³å®šç‰ˆ Seq2Seq è®­ç»ƒæ•°æ®é›†"""

    def __init__(self, data_path, tokenizer, max_length=512, max_target_length=256):
        """
        Args:
            data_path: æ•°æ®æ–‡ä»¶è·¯å¾„ï¼Œæ¯è¡Œæ ¼å¼ï¼šinput <SEP> output
            tokenizer: HuggingFace tokenizer (T5Tokenizer)
            max_length: è¾“å…¥æœ€å¤§é•¿åº¦
            max_target_length: target æœ€å¤§é•¿åº¦ï¼ˆä»…ç”¨äº truncationï¼Œä¸ paddingï¼‰
        """
        self.tokenizer = tokenizer
        self.max_length = max_length
        self.max_target_length = max_target_length

        print(f"ğŸ“¥ æ­£åœ¨è¯»å–æ•°æ®æ–‡ä»¶: {data_path}")
        with open(data_path, 'r', encoding='utf-8') as f:
            self.samples = [line.strip() for line in f if line.strip()]

        print(f"âœ… å…±åŠ è½½ {len(self.samples)} æ¡æ ·æœ¬")

    def __len__(self):
        return len(self.samples)

    # ============================
    # æ ¸å¿ƒï¼šç¨³å®š target æ„é€ é€»è¾‘
    # ============================
    def _normalize_output(self, output_text: str) -> str:
        """
        ä¿è¯ targetï¼š
        1. æ°¸è¿œéç©º
        2. ç»“æ„ç¨³å®šï¼ˆåŒ¹é…å®é™…æ•°æ®æ ¼å¼ï¼šä¸‰å…ƒç»„åˆ—è¡¨ï¼‰
        3. token æ•°ä¸å°‘äºå®‰å…¨çº¿
        """
        output_text = output_text.strip()

        # æƒ…å†µ 1ï¼šå®Œå…¨ç©ºæˆ–åªæœ‰ç©ºç™½
        if output_text == "" or not output_text:
            # è¿”å›ä¸€ä¸ªæœ‰æ•ˆçš„ç©ºä¸‰å…ƒç»„æ ¼å¼ï¼ˆä¿æŒæ ¼å¼ä¸€è‡´æ€§ï¼‰
            output_text = "(NONE, NONE, NONE)"

        # æƒ…å†µ 2ï¼šåªæœ‰ NONEï¼ˆå¤§å°å†™ä¸æ•æ„Ÿï¼‰
        elif output_text.upper().strip() == "NONE":
            output_text = "(NONE, NONE, NONE)"

        # æƒ…å†µ 3ï¼šå·²æœ‰ä¸‰å…ƒç»„æ ¼å¼ï¼ˆé»˜è®¤ä¿¡ä»»ä¸Šæ¸¸ï¼‰
        # å®é™…æ ¼å¼ç¤ºä¾‹: (å®ä½“1, å…³ç³», å®ä½“2); (å®ä½“3, å…³ç³», å®ä½“4)
        # è¿™é‡Œå¯ä»¥æ·»åŠ æ›´ä¸¥æ ¼çš„æ ¼å¼æ ¡éªŒï¼Œä½†ä¸ºäº†ç¨³å®šæ€§ï¼Œå…ˆä¿¡ä»»ä¸Šæ¸¸æ•°æ®

        # ============================
        # token æ•°å®‰å…¨ä¿æŠ¤ï¼ˆéå¸¸é‡è¦ï¼‰
        # ç¡®ä¿æœ‰è¶³å¤Ÿçš„ token é¿å… FP16 ç²¾åº¦é—®é¢˜
        # ============================
        token_len = len(self.tokenizer.tokenize(output_text))
        if token_len < 5:
            # å·¥ç¨‹å…œåº•ï¼šå¦‚æœ token æ•°å¤ªå°‘ï¼Œè¡¥å……ä¸€äº› padding
            # æ³¨æ„ï¼šè¿™é‡Œä¸æ·»åŠ  <PAD> tokenï¼Œå› ä¸ºä¼šè¢« mask æ‰
            # è€Œæ˜¯æ·»åŠ ä¸€äº›ä¸å½±å“è¯­ä¹‰çš„å ä½ç¬¦
            output_text = output_text + " . . ."
        
        return output_text

    def __getitem__(self, idx):
        line = self.samples[idx]

        # ----------------------------
        # 1. æ‹†åˆ† input / output
        # ----------------------------
        if ' <SEP> ' in line:
            input_text, output_text = line.split(' <SEP> ', 1)
        else:
            input_text = line
            output_text = ""

        # ----------------------------
        # 2. æ ‡å‡†åŒ– targetï¼ˆå…³é”®ï¼‰
        # ----------------------------
        output_text = self._normalize_output(output_text)

        # ----------------------------
        # 3. Tokenize inputï¼ˆpadding åˆ° max_lengthï¼‰
        # ----------------------------
        input_enc = self.tokenizer(
            input_text,
            max_length=self.max_length,
            truncation=True,
            padding='max_length',
            return_tensors=None,
        )

        # ----------------------------
        # 4. Tokenize targetï¼ˆä¸ paddingï¼‰
        # ----------------------------
        target_enc = self.tokenizer(
            output_text,
            max_length=self.max_target_length,
            truncation=True,
            padding=False,  # å…³é”®ï¼šä¸åœ¨ Dataset é˜¶æ®µ padding
            return_tensors=None,
        )

        labels = np.array(target_enc['input_ids'], dtype=np.int64)

        # å°† pad token mask ä¸º -100ï¼ˆå¿½ç•¥ lossï¼‰
        pad_token_id = (
            self.tokenizer.pad_token_id
            if self.tokenizer.pad_token_id is not None
            else self.tokenizer.eos_token_id
        )
        labels[labels == pad_token_id] = -100

        return {
            'input_ids': np.array(input_enc['input_ids'], dtype=np.int64),
            'attention_mask': np.array(input_enc['attention_mask'], dtype=np.int64),
            'labels': labels,
        }


# ============================
# æ¨ç†æ•°æ®é›†ï¼ˆå¯é€‰ï¼‰
# ============================
class Seq2SeqInferenceDataset(Dataset):
    def __init__(self, texts, tokenizer, max_length=512):
        self.texts = texts
        self.tokenizer = tokenizer
        self.max_length = max_length

    def __len__(self):
        return len(self.texts)

    def __getitem__(self, idx):
        text = self.texts[idx]
        enc = self.tokenizer(
            text,
            max_length=self.max_length,
            truncation=True,
            padding='max_length',
            return_tensors=None,
        )
        return {
            'input_ids': np.array(enc['input_ids'], dtype=np.int64),
            'attention_mask': np.array(enc['attention_mask'], dtype=np.int64),
        }